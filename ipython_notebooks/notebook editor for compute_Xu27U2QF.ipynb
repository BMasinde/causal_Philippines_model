{
  "metadata": {
    "kernelspec": {
      "name": "r-dku-venv-r_env",
      "display_name": "R (env R_env)",
      "language": "R"
    },
    "hide_input": false,
    "language_info": {
      "name": "R",
      "codemirror_mode": "r",
      "pygments_lexer": "r",
      "mimetype": "text/x-r-source",
      "file_extension": ".r",
      "version": "4.4.1"
    },
    "createdOn": 1740387212880,
    "associatedRecipe": "compute_Xu27U2QF",
    "creationTag": {
      "versionNumber": 0,
      "lastModifiedBy": {
        "login": "admin"
      },
      "lastModifiedOn": 1740387212880
    },
    "customFields": {},
    "creator": "admin",
    "tags": [
      "recipe-editor"
    ],
    "dkuGit": {
      "lastInteraction": 0
    },
    "modifiedBy": "admin"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Objective of recipe is to:\n# Predict on the scm_min_clas_model on the test set\n# Get the classification metrics on the test set"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "library(dataiku)\nlibrary(rpart)\nlibrary(caret)\nlibrary(pROC) # For AUC calculation\nlibrary(dplyr)\nlibrary(data.table)\nlibrary(mlflow)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Recipe inputs\nfolder_path \u003c- dkuManagedFolderPath(\"xcPrnvPS\")\nbase_test \u003c- dkuReadDataset(\"base_test\", samplingMethod\u003d\"head\", nbRows\u003d100000)\n\n\n# Construct the full file paths for the models\nclas_file_path \u003c- file.path(folder_path, \"base_clas_full_model.rds\")\nwind_file_path  \u003c- file.path(folder_path, \"base_wind_model.rds\")\nrain_file_path  \u003c- file.path(folder_path, \"base_rain_model.rds\")\ntrack_file_path  \u003c- file.path(folder_path, \"base_track_model.rds\")\n\n\n# read the .rds model\nbase_clas_full_model  \u003c- readRDS(clas_file_path)\nbase_wind_model  \u003c- readRDS(wind_file_path)\nbase_rain_model  \u003c- readRDS(rain_file_path)\nbase_track_model  \u003c- readRDS(track_file_path)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# predicting track_min_dist, wind_max and rain \u0026 updating the base_test to df_base_test\n\ndf_base_test  \u003c- base_test %\u003e%\n    mutate(\n    track_min_dist_pred \u003d predict(base_track_model, newdata \u003d base_test),\n    wind_max_pred \u003d predict(base_wind_model, newdata \u003d base_test),\n    rain_total_pred \u003d predict(base_rain_model, newdata \u003d base_test),\n    wind_blue_ss \u003d wind_max_pred * blue_ss_frac, # Updating interaction terms\n    wind_yellow_ss \u003d wind_max_pred * yellow_ss_frac,\n    wind_orange_ss \u003d wind_max_pred * orange_ss_frac,\n    wind_red_ss \u003d wind_max_pred * red_ss_frac,\n    rain_blue_ss \u003d rain_total_pred * blue_ls_frac,\n    rain_yellow_ss \u003d rain_total_pred * yellow_ls_frac,\n    rain_orange_ss \u003d rain_total_pred * orange_ls_frac,\n    rain_red_ss \u003d rain_total_pred * red_ls_frac,    \n    )"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# predict for damage_binary\n# Make probability predictions for classification\ny_preds_probs \u003c- predict(base_clas_full_model, newdata \u003d df_base_test, type \u003d \"prob\")[,2]  # Probability of class 1\n#y_preds_probs"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# AUC\n# Compute AUC (better for classification)\nauc_value \u003c- auc(roc(df_base_test$damage_binary, y_preds_probs))\nauc_value"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# extracting probability that y_pred \u003d\u003d 1\n#y_preds_prob_1 \u003c- y_preds_prob[ ,2]\n\n## assigning final class based on threshold\ny_pred \u003c- ifelse(y_preds_prob \u003e 0.5, 1, 0)\n\n# using table function\nconf_matrix \u003c- confusionMatrix(as.factor(y_pred),\n                     as.factor(df_base_test$damage_binary),\n                     positive \u003d \"1\"\n                     )\nprint(conf_matrix)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "base_clas_full_model$bestTune"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# logging in mflow:\n# Logging the model and parameter using MLflow\n\n# set tracking URI\nmlflow_set_tracking_uri(\"http://127.0.0.1:5000\")\n\n# Ensure any active run is ended\nsuppressWarnings(try(mlflow_end_run(), silent \u003d TRUE))\n\n# set experiment\n# Logging metrics for model training and the parameters used\nmlflow_set_experiment(experiment_name \u003d \"SCM - XGBOOST classification (Test metircs)\")\n\n# Ensure that MLflow has only one run. Start MLflow run once.\nrun_name \u003c- paste(\"XGBoost Run\", Sys.time())  # Unique name using current time\n\n\n# Start MLflow run\nmlflow_start_run(nested \u003d FALSE)\n\n# Ensure the run ends even if an error occurs\n#on.exit(mlflow_end_run(), add \u003d TRUE)\n\n# Extract the best parameters (remove AUC column)\n#best_params_model \u003c- best_params %\u003e% # Remove AUC column if present\n#    select(-AUC)\n\nparameters_used  \u003c- base_clas_full_model$bestTune\n\n# Log each of the best parameters in MLflow\nfor (param in names(parameters_used)) {\n  mlflow_log_param(param, parameters_used[[param]])\n}\n\n# Log the model type as a parameter\nmlflow_log_param(\"model_type\", \"scm-xgboost-classification\")\n\n# predicting\ny_preds_probs \u003c- predict(base_clas_full_model, newdata \u003d df_base_test, type \u003d \"prob\")[,2]  # Probability of class 1\ny_pred \u003c- ifelse(y_preds_prob \u003e 0.5, 1, 0)\n\n# summarize results\nconf_matrix \u003c- confusionMatrix(as.factor(y_pred),\n                     as.factor(df_base_test$damage_binary),  \n                     positive \u003d \"1\"\n                     )\n\n# accuracy\naccuracy  \u003c- conf_matrix$overall[\u0027Accuracy\u0027]\n\n# Positive class \u003d 1, precision, recall, and F1\n# Extract precision, recall, and F1 score\nprecision \u003c- conf_matrix$byClass[\u0027Precision\u0027]\nrecall \u003c- conf_matrix$byClass[\u0027Recall\u0027]\nf1_score \u003c- conf_matrix$byClass[\u0027F1\u0027]\n\n\n# Log parameters and metrics\n# mlflow_log_param(\"model_type\", \"scm-xgboost-classification\")\nmlflow_log_metric(\"accuracy\", accuracy)\nmlflow_log_metric(\"F1\", f1_score)\nmlflow_log_metric(\"Precision\", precision)\nmlflow_log_metric(\"Recall\", recall)\n\n\n# Save model\n#saveRDS(model, file \u003d file.path(path_2_folder, \"spam_clas_model.rds\"))\n\n# End MLflow run\nmlflow_end_run()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Extract recall and precision\n# Compute confusion matrix\nconf_matrix \u003c- confusionMatrix(as.factor(y_pred), as.factor(df_base_test$damage_binary), positive \u003d \"1\")\nrecall \u003c- conf_matrix$byClass[\"Sensitivity\"]  # Recall (Sensitivity)\nprecision \u003c- conf_matrix$byClass[\"Precision\"] # Precision\nf1_score  \u003c- conf_matrix$byClass[\"F1\"]\naccuracy  \u003c- conf_matrix$overall[\u0027Accuracy\u0027]\n\n# metrics in a table\n# Create a data frame with the metrics\nmetrics_df \u003c- data.frame(\n  Metric \u003d c(\"Accuracy\", \"Recall\", \"Precision\", \"F1\"),\n  Value \u003d c(accuracy, recall, precision, f1_score)\n)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "metrics_df"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Recipe outputs\nmetrics_folder_path \u003c- dkuManagedFolderPath(\"Xu27U2QF\")\n\n# Saving the predicted values\n# Define file path\nfile_path \u003c- file.path(metrics_folder_path, \"model_metrics.csv\")\n\n# Write to CSV\nfwrite(metrics_df, file \u003d file_path, row.names \u003d FALSE)\n\n#dkuWriteDataset(metrics_df, \"min_clas_metrics_df\")\n\n# Print message to confirm\nprint(paste(\"Metrics saved to:\", metrics_folder_path))"
      ],
      "outputs": []
    }
  ]
}