{
  "metadata": {
    "kernelspec": {
      "name": "r-dku-venv-r_env",
      "display_name": "R (env R_env)",
      "language": "R"
    },
    "hide_input": false,
    "language_info": {
      "name": "R",
      "codemirror_mode": "r",
      "pygments_lexer": "r",
      "mimetype": "text/x-r-source",
      "file_extension": ".r",
      "version": "4.4.1"
    },
    "createdOn": 1740387212880,
    "associatedRecipe": "compute_Xu27U2QF",
    "creationTag": {
      "versionNumber": 0,
      "lastModifiedBy": {
        "login": "admin"
      },
      "lastModifiedOn": 1740387212880
    },
    "customFields": {},
    "creator": "admin",
    "tags": [
      "recipe-editor"
    ],
    "dkuGit": {
      "lastInteraction": 0
    },
    "modifiedBy": "admin"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Objective of recipe is to:\n# Predict on the scm_min_clas_model on the test set\n# Get the classification metrics on the test set"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "library(dataiku)\nlibrary(rpart)\nlibrary(caret)\nlibrary(pROC) # For AUC calculation\nlibrary(dplyr)\nlibrary(data.table)\nlibrary(mlflow)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Recipe inputs\nfolder_path \u003c- dkuManagedFolderPath(\"xcPrnvPS\")\nbase_test \u003c- dkuReadDataset(\"base_test\", samplingMethod\u003d\"head\", nbRows\u003d100000)\n\n\n# Construct the full file paths for the models\nclas_file_path \u003c- file.path(folder_path, \"base_clas_full_model.rds\")\nwind_file_path  \u003c- file.path(folder_path, \"base_wind_model.rds\")\nrain_file_path  \u003c- file.path(folder_path, \"base_rain_model.rds\")\n\n\n# read the .rds model\nbase_clas_full_model  \u003c- readRDS(clas_file_path)\nbase_wind_model  \u003c- readRDS(wind_file_path)\nbase_rain_model  \u003c- readRDS(rain_file_path)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# predicting wind_max and rain \u0026 updating the base_test to df_base_test\n\ndf_base_test  \u003c- base_test %\u003e%\n    mutate(\n    wind_max_pred \u003d predict(\n      base_wind_model, newdata \u003d base_test),\n    rain_total_pred \u003d predict(\n      base_rain_model,\n      newdata \u003d base_test)\n    )"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# predict for damage_binary\n# Make probability predictions for classification\ny_preds \u003c- predict(base_clas_full_model, newdata \u003d df_base_test, type \u003d \"prob\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# AUC\n# Compute AUC (better for classification)\nauc_value \u003c- auc(roc(df_base_test$damage_binary, y_preds_prob))\nauc_value"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# extracting probability that y_pred \u003d\u003d 1\n#y_preds_prob_1 \u003c- y_preds_prob[ ,2]\n\n## assigning final class based on threshold\ny_pred \u003c- ifelse(y_preds_prob \u003e 0.5, 1, 0)\n\n# using table function\nconf_matrix \u003c- table(predicted \u003d y_pred,\n                     actual \u003d df_base_test$damage_binary\n                     )\nprint(conf_matrix)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracy \u003c- sum(diag(conf_matrix)) / sum(conf_matrix)\n\ncat(\"test-set accuracy of minimal CLASSIFICATION SCM model:\", accuracy, sep \u003d \" \")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Extract recall and precision\n# Compute confusion matrix\nconf_matrix \u003c- confusionMatrix(as.factor(y_pred), as.factor(df_base_test$damage_binary), positive \u003d \"1\")\nrecall \u003c- conf_matrix$byClass[\"Sensitivity\"]  # Recall (Sensitivity)\nprecision \u003c- conf_matrix$byClass[\"Precision\"] # Precision\n\n# metrics in a table\n# Create a data frame with the metrics\nmetrics_df \u003c- data.frame(\n  Metric \u003d c(\"Accuracy\", \"Recall\", \"Precision\"),\n  Value \u003d c(accuracy, recall, precision)\n)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "metrics_df"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Recipe outputs\nmetrics_folder_path \u003c- dkuManagedFolderPath(\"Xu27U2QF\")\n\n# Saving the predicted values\n# Define file path\nfile_path \u003c- file.path(metrics_folder_path, \"model_metrics.csv\")\n\n# Write to CSV\nfwrite(metrics_df, file \u003d file_path, row.names \u003d FALSE)\n\n#dkuWriteDataset(metrics_df, \"min_clas_metrics_df\")\n\n# Print message to confirm\nprint(paste(\"Metrics saved to:\", metrics_folder_path))"
      ],
      "outputs": []
    }
  ]
}