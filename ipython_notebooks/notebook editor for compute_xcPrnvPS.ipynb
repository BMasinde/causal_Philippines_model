{
  "metadata": {
    "kernelspec": {
      "name": "r-dku-venv-r_env",
      "display_name": "R (env R_env)",
      "language": "R"
    },
    "hide_input": false,
    "language_info": {
      "name": "R",
      "codemirror_mode": "r",
      "pygments_lexer": "r",
      "mimetype": "text/x-r-source",
      "file_extension": ".r",
      "version": "4.4.1"
    },
    "createdOn": 1740376599429,
    "associatedRecipe": "compute_xcPrnvPS",
    "creationTag": {
      "versionNumber": 0,
      "lastModifiedBy": {
        "login": "admin"
      },
      "lastModifiedOn": 1740376599429
    },
    "customFields": {},
    "creator": "admin",
    "tags": [
      "recipe-editor"
    ],
    "dkuGit": {
      "lastInteraction": 0
    },
    "modifiedBy": "admin"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Libraries\nlibrary(dataiku)\nlibrary(rpart)\nlibrary(dplyr)\nlibrary(caret)\nlibrary(pROC) # For AUC calculation\nlibrary(data.table)\nlibrary(mlflow)\nlibrary(reticulate)\nlibrary(Matrix)\nlibrary(purrr) # useful for code optimization"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Recipe inputs\ndf_base_train \u003c- dkuReadDataset(\"base_train\", samplingMethod\u003d\"head\", nbRows\u003d100000)\ndf_base_validation \u003c- dkuReadDataset(\"base_validation\", samplingMethod\u003d\"head\", nbRows\u003d100000)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Combining train and validation datasets to one\n# Because we are going to use CV to train the models later\n# naming it df_base_train2 to remain consistent with df naming\ndf_base_train2  \u003c- rbind(df_base_train, df_base_validation)\n\ncat(\"number of rows in combined train data:\", nrow(df_base_train2), sep \u003d \" \")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Training track_min_dist ~ island_groups\n# we will need to also include island_groups\n# in the final outcome prediction model to adjust for the confounding\n\nbase_track_model  \u003c- rpart(track_min_dist  ~ island_groups,\n                          data \u003d df_base_train2,\n                          method \u003d \"anova\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Training structural equation for wind speed\n# wind_speed \u003d f(track_min_dist, eps)\n\n\nbase_wind_model \u003c- rpart(wind_max ~ track_min_dist,\n                       data \u003d df_base_train2,\n                       method \u003d \"anova\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Training structural equation for rain speed\n# rain_total \u003d f(track_min_dist, eps)\n\nbase_rain_model \u003c- rpart(rain_total ~ track_min_dist,\n                       data \u003d df_base_train2,\n                       method \u003d \"anova\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Building typologies are determined by region\nbase_roof_strong_wall_strong_model  \u003c- rpart(roof_strong_wall_strong  ~ island_groups, \n                                             data \u003d df_base_train2,\n                                            method \u003d \"anova\")\n\nbase_roof_strong_wall_light_model  \u003c- rpart(roof_strong_wall_light ~ island_groups,\n                                           data \u003d df_base_train2,\n                                           method \u003d \"anova\")\n\nbase_roof_strong_wall_salv_model  \u003c- rpart(roof_strong_wall_salv ~ island_groups,\n                                          data \u003d df_base_train2,\n                                          method \u003d \"anova\")\nbase_roof_light_wall_strong_model  \u003c- rpart(roof_light_wall_strong ~ island_groups,\n                                           data \u003d df_base_train2,\n                                           method \u003d \"anova\")\nbase_roof_light_wall_light_model  \u003c- rpart(roof_light_wall_light ~ island_groups,\n                                          data \u003d df_base_train2,\n                                          method \u003d \"anova\")\nbase_roof_light_wall_salv_model  \u003c- rpart(roof_light_wall_salv ~ island_groups,\n                                         data \u003d df_base_train2,\n                                         method \u003d \"anova\")\n\nbase_roof_salv_wall_strong_model  \u003c- rpart(roof_salv_wall_strong ~ island_groups,\n                                          data \u003d df_base_train2,\n                                          method \u003d \"anova\")\n\nbase_roof_salv_wall_light_model  \u003c- rpart(roof_salv_wall_light ~ island_groups,\n                                  data \u003d df_base_train2,\n                                  method \u003d \"anova\")\n\nbase_roof_salv_wall_salv_model  \u003c- rpart(roof_salv_wall_salv ~ island_groups,\n                                  data \u003d df_base_train2,\n                                  method \u003d \"anova\")\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# OPTMIZED CODE\n# Define models in a named list\nmodel_list \u003c- list(\n  track_min_dist \u003d base_track_model,\n  wind_max \u003d base_wind_model,\n  rain_total \u003d base_rain_model,\n  roof_strong_wall_strong \u003d base_roof_strong_wall_strong_model,\n  roof_strong_wall_light \u003d base_roof_strong_wall_light_model,\n  roof_strong_wall_salv \u003d base_roof_strong_wall_salv_model,\n  roof_light_wall_strong \u003d base_roof_light_wall_strong_model,\n  roof_light_wall_light \u003d base_roof_light_wall_light_model,\n  roof_light_wall_salv \u003d base_roof_light_wall_salv_model,\n  roof_salv_wall_strong \u003d base_roof_salv_wall_strong_model,\n  roof_salv_wall_light \u003d base_roof_salv_wall_light_model,\n  roof_salv_wall_salv \u003d base_roof_salv_wall_salv_model\n)\n\n# Apply predictions efficiently\ndf_base_train2 \u003c- df_base_train2 %\u003e%\n  mutate(across(names(model_list), ~ predict(model_list[[cur_column()]], newdata \u003d df_base_train2), .names \u003d \"{.col}_pred\")) \n\n# Define wind and rain interaction variables\nwind_fractions \u003c- c(\"blue_ss_frac\", \"yellow_ss_frac\", \"orange_ss_frac\", \"red_ss_frac\")\nrain_fractions \u003c- c(\"blue_ls_frac\", \"yellow_ls_frac\", \"orange_ls_frac\", \"red_ls_frac\")\n\n# Compute wind interaction terms dynamically\ndf_base_train2 \u003c- df_base_train2 %\u003e%\n  mutate(across(all_of(wind_fractions), ~ . * wind_max_pred, .names \u003d \"wind_{.col}\"),\n         across(all_of(rain_fractions), ~ . * rain_total_pred, .names \u003d \"rain_{.col}\"))"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "code_folding": [
          0
        ]
      },
      "source": [
        "# Adding the predicted parents\u0027 to the training dataset\n\n# df_train \u003c- df_train %\u003e%\n#   mutate(track_min_dist_pred \u003d predict(base_track_model, newdata \u003d df_base_train2), # predicted min_dist\n#          wind_max_pred \u003d predict(base_wind_model, newdata \u003d df_base_train2),\n#          rain_total_pred \u003d predict(base_rain_model, newdata \u003d df_base_train2), \n#          #---- Updating interaction terms ------------------------\n#          wind_blue_ss \u003d wind_max_pred * blue_ss_frac,\n#          wind_yellow_ss \u003d wind_max_pred * yellow_ss_frac,\n#          wind_orange_ss \u003d wind_max_pred * orange_ss_frac,\n#          wind_red_ss \u003d wind_max_pred * red_ss_frac,\n#          rain_blue_ss \u003d rain_total_pred * blue_ls_frac,\n#          rain_yellow_ss \u003d rain_total_pred * yellow_ls_frac,\n#          rain_orange_ss \u003d rain_total_pred * orange_ls_frac,\n#          rain_red_ss \u003d rain_total_pred * red_ls_frac,\n#          # -------- Updating building typologies ------------------\n#          roof_strong_wall_strong_pred \u003d predict(base_roof_strong_wall_strong_model, newdata \u003d df_base_train2), \n#          roof_strong_wall_light_pred \u003d predict(base_roof_strong_wall_light_model, newdata \u003d df_base_train2),\n#          roof_strong_wall_salv_pred \u003d predict(base_roof_strong_wall_salv_model, newdata \u003d df_base_train2),\n#          roof_light_wall_strong_pred \u003d predict(base_roof_light_wall_strong_model, newdata \u003d df_base_train2),\n#          roof_light_wall_light_pred \u003d predict(base_roof_light_wall_light_model, newdata \u003d df_base_train2),\n#          roof_light_wall_salv_pred \u003d predict(base_roof_light_wall_salv_model, newdata \u003d df_base_train2),\n#          roof_salv_wall_strong_pred \u003d predict(base_roof_salv_wall_strong_model, newdata \u003d df_base_train2),\n#          roof_salv_wall_light_pred \u003d predict(base_roof_salv_wall_light_model, newdata \u003d df_base_train2),\n#          roof_salv_wall_salv_pred \u003d predict(base_roof_salv_wall_salv_model, newdata \u003d df_base_train2), \n#          )"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "code_folding": [
          0
        ]
      },
      "source": [
        "#--------------------------- NOT NEEDED BECAUSE WE OPT TO DO CV IN TRAINING -------------------------------------\n# Adding the predicted parents\u0027 to the validation dataset\n# predicting for wind and rainfall for the validation dataset\n#df_base_validation \u003c- df_base_validation %\u003e%\n#  mutate(track_min_dist_pred \u003d predict(base_track_model, newdata \u003d df_base_validation),  # First predict for track_min_dist from regions\n#    wind_max_pred \u003d predict(base_wind_model, newdata \u003d df_base_validation),\n#    rain_total_pred \u003d predict(base_rain_model, newdata \u003d df_base_validation),\n#    wind_blue_ss \u003d wind_max_pred * blue_ss_frac,\n#    wind_yellow_ss \u003d wind_max_pred * yellow_ss_frac,\n#    wind_orange_ss \u003d wind_max_pred * orange_ss_frac,\n#    wind_red_ss \u003d wind_max_pred * red_ss_frac,\n#    rain_blue_ss \u003d rain_total_pred * blue_ls_frac,\n#    rain_yellow_ss \u003d rain_total_pred * yellow_ls_frac,\n#    rain_orange_ss \u003d rain_total_pred * orange_ls_frac,\n#    rain_red_ss \u003d rain_total_pred * red_ls_frac,\n#  )"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "code_folding": [
          0
        ]
      },
      "source": [
        "# ------------------ GRID SEARCH TUNING ------------------------------------\n# # Parameter tuning\n\n# # Define tuning grid\n# tune_grid \u003c- expand.grid(\n#   nrounds \u003d c(50, 100, 150),       # Number of boosting rounds\n#   max_depth \u003d c(3, 6, 9),          # Maximum tree depth\n#   eta \u003d c(0.01, 0.1, 0.3),         # Learning rate\n#   gamma \u003d 0,                       # Minimum loss reduction\n#   colsample_bytree \u003d 0.8,          # Feature selection rate\n#   min_child_weight \u003d 1,            # Minimum instance weight\n#   subsample \u003d 0.8                  # Sample ratio per boosting round\n# )\n\n\n# # Create an empty list to store results\n# results_list \u003c- list()\n\n# # Extra data prep\n# # Ensure target variable is a factor for classification\n# df_base_train2$damage_binary \u003c- as.factor(df_base_train2$damage_binary)\n# #df_base_validation$damage_binary \u003c- as.factor(df_base_validation$damage_binary)\n\n# # Train the model using manual grid search\n# grid_id \u003c- 1  # Index for list storage\n\n# # Iterate over all combinations of hyperparameters\n# for (i in 1:nrow(tune_grid)) {\n#   params \u003c- tune_grid[i, ]\n\n#         # setting seed for reproducibility\n#         set.seed(1234)\n#         # Train the model with specific hyperparameters\n#         xgb_model \u003c- train(\n#           as.factor(damage_binary) ~ wind_max_pred +\n#             rain_total_pred +\n#             roof_strong_wall_strong +\n#             roof_strong_wall_light +\n#             roof_strong_wall_salv +\n#             roof_light_wall_strong +\n#             roof_light_wall_light +\n#             roof_light_wall_salv +\n#             roof_salv_wall_strong +\n#             roof_salv_wall_light +\n#             roof_salv_wall_salv +\n#             ls_risk_pct +\n#             ss_risk_pct +\n#             wind_blue_ss +\n#             wind_yellow_ss +\n#             wind_orange_ss +\n#             wind_red_ss +\n#             rain_blue_ss +\n#             rain_yellow_ss +\n#             rain_orange_ss +\n#             rain_red_ss +\n#             island_groups, # CONFOUNDER ADJUSTED\n#           data \u003d df_base_train,\n#           method \u003d \"xgbTree\", # XGBoost method\n#           trControl \u003d trainControl(method \u003d \"none\"),  # No automatic validation\n#           tuneGrid \u003d params # Hyperparameter grid\n#         )\n\n#         # Make probability predictions for classification\n#         val_predictions \u003c- predict(xgb_model, newdata \u003d df_base_validation, type \u003d \"prob\")[,2]  # Probability of class 1\n\n#         # Compute AUC (better for classification)\n#         auc_value \u003c- auc(df_base_validation$damage_binary, val_predictions)\n\n#         # Store results efficiently in a list\n#         results_list[[i]] \u003c- data.frame(params, AUC \u003d auc_value)\n# }\n\n# # Convert list to data frame\n# results \u003c- rbindlist(results_list)\n\n# # Print the best hyperparameter combination (highest AUC)\n# best_params \u003c- results[which.max(results$AUC), ]\n# print(best_params)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure target variable is a factor\n# Ensure the target variable is a factor with valid names\n\n#df_base_train2$damage_binary \u003c- as.factor(df_base_train2$damage_binary)\n\ndf_base_train2$damage_binary_2 \u003c- factor(df_base_train2$damage_binary, \n                                       levels \u003d c(\"0\", \"1\"),  # Your current levels\n                                       labels \u003d c(\"Damage_below_10\", \"Damage_above_10\"))  # New valid labels\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------- CLASSIFICATION MDOEL TRAINING WITH 10 CV \u0026 GRID SEARCH PARAMETER TUNING -----------------\n# Define tuning grid\ntune_grid \u003c- expand.grid(\n  nrounds \u003d c(50, 100, 200, 300, 400, 500),\n  max_depth \u003d c(3, 6, 9, 12),\n  eta \u003d c(0.01, 0.05, 0.1, 0.2, 0.3),\n  gamma \u003d c(0, 1, 5, 10),\n  colsample_bytree \u003d c(0.5, 0.7, 0.8, 1.0),\n  min_child_weight \u003d c(1, 3, 5, 10),\n  subsample \u003d c(0.5, 0.7, 0.8, 1.0)\n)\n\n\n# Set up train control with 10-fold cross-validation\ntrain_control \u003c- trainControl(\n  method \u003d \"cv\",\n  number \u003d 10,\n  classProbs \u003d TRUE,  # Needed for AUC calculation\n  summaryFunction \u003d twoClassSummary\n)\n\n# Train the model using grid search with 10-fold CV\nset.seed(1234)\nxgb_model \u003c- train(\n  damage_binary_2 ~ wind_max_pred +\n    rain_total_pred +\n    roof_strong_wall_strong_pred +\n    roof_strong_wall_light_pred +\n    roof_strong_wall_salv_pred +\n    roof_light_wall_strong_pred +\n    roof_light_wall_light_pred +\n    roof_light_wall_salv_pred +\n    roof_salv_wall_strong_pred +\n    roof_salv_wall_light_pred +\n    roof_salv_wall_salv_pred +\n    ls_risk_pct +\n    ss_risk_pct +\n    wind_blue_ss +\n    wind_yellow_ss +\n    wind_orange_ss +\n    wind_red_ss +\n    rain_blue_ss +\n    rain_yellow_ss +\n    rain_orange_ss +\n    rain_red_ss +\n    island_groups +  # Confounder adjustment\n    track_min_dist_pred, # Confounder adjustment\n  data \u003d df_base_train2,\n  method \u003d \"xgbTree\",\n  trControl \u003d train_control,\n  tuneGrid \u003d tune_grid,\n  metric \u003d \"ROC\"  # Optimize based on AUC\n)\n\n# Print best parameters\nprint(xgb_model$bestTune)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Training based on tuned parameters\n\n# Combine Training and Validation datasets for final training\n\nfinal_training_df  \u003c- rbind(df_base_train,\n                           df_base_validation)\n\n\n# Extract the best parameters (remove AUC column)\nbest_params_model \u003c- best_params %\u003e% # Remove AUC column if present\n    select(-AUC)\n\ndamage_fit_class_full \u003c- train(\n          as.factor(damage_binary) ~ wind_max_pred +\n            rain_total_pred +\n            roof_strong_wall_strong +\n            roof_strong_wall_light +\n            roof_strong_wall_salv +\n            roof_light_wall_strong +\n            roof_light_wall_light +\n            roof_light_wall_salv +\n            roof_salv_wall_strong +\n            roof_salv_wall_light +\n            roof_salv_wall_salv +\n            ls_risk_pct +\n            ss_risk_pct +\n            wind_blue_ss +\n            wind_yellow_ss +\n            wind_orange_ss +\n            wind_red_ss +\n            rain_blue_ss +\n            rain_yellow_ss +\n            rain_orange_ss +\n            rain_red_ss +\n            island_groups,\n          data \u003d final_training_df, # USE TRAINING AND VALIDATION SETS COMBINED\n          method \u003d \"xgbTree\", # XGBoost method\n          trControl \u003d trainControl(method \u003d \"none\"),  # No automatic validation\n          tuneGrid \u003d best_params_model # USE BEST PARAMETER\n        )"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sanity Check\n# testing on the training datasets (training + validation)\n\n## Outcome prediction on the final_training_df dataset\n## default function predict returns class probabilities (has two columns)\ny_pred \u003c- predict(damage_fit_class_full,\n                  newdata \u003d final_training_df)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# using table function\nconf_matrix \u003c- confusionMatrix(y_pred,\n                     final_training_df$damage_binary,\n                     positive \u003d \"1\"\n                     )\nconf_matrix"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracy \u003c- conf_matrix$overall[\u0027Accuracy\u0027]\n\ncat(\"test-set accuracy of minimal SCM model:\", accuracy, sep \u003d \" \")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Logging the model and parameter using MLflow\n\n# set tracking URI\nmlflow_set_tracking_uri(\"http://127.0.0.1:5000\")\n\n# Ensure any active run is ended\nsuppressWarnings(try(mlflow_end_run(), silent \u003d TRUE))\n\n# set experiment\n# Logging metrics for model training and the parameters used\nmlflow_set_experiment(experiment_name \u003d \"SCM - XGBOOST classification (Training metircs)\")\n\n# Ensure that MLflow has only one run. Start MLflow run once.\nrun_name \u003c- paste(\"XGBoost Run\", Sys.time())  # Unique name using current time\n\n\n# Start MLflow run\nmlflow_start_run(nested \u003d FALSE)\n\n# Ensure the run ends even if an error occurs\n#on.exit(mlflow_end_run(), add \u003d TRUE)\n\n# Extract the best parameters (remove AUC column)\nbest_params_model \u003c- best_params %\u003e% # Remove AUC column if present\n    select(-AUC)\n\n# Log each of the best parameters in MLflow\nfor (param in names(best_params_model)) {\n  mlflow_log_param(param, best_params_model[[param]])\n}\n\n# Log the model type as a parameter\nmlflow_log_param(\"model_type\", \"scm-xgboost-classification\")\n\ndamage_fit_class_full \u003c- xgb_model \u003c- train(\n          as.factor(damage_binary) ~ wind_max_pred +\n            rain_total_pred +\n            roof_strong_wall_strong +\n            roof_strong_wall_light +\n            roof_strong_wall_salv +\n            roof_light_wall_strong +\n            roof_light_wall_light +\n            roof_light_wall_salv +\n            roof_salv_wall_strong +\n            roof_salv_wall_light +\n            roof_salv_wall_salv +\n            ls_risk_pct +\n            ss_risk_pct +\n            wind_blue_ss +\n            wind_yellow_ss +\n            wind_orange_ss +\n            wind_red_ss +\n            rain_blue_ss +\n            rain_yellow_ss +\n            rain_orange_ss +\n            rain_red_ss +\n            island_groups,\n          data \u003d final_training_df, # USE TRAINING AND VALIDATION SETS COMBINED\n          method \u003d \"xgbTree\", # XGBoost method\n          trControl \u003d trainControl(method \u003d \"none\"),  # No automatic validation\n          tuneGrid \u003d best_params_model # USE BEST PARAMETER\n        )\n\n\n# summarize results\nconf_matrix \u003c- confusionMatrix(y_pred,\n                     final_training_df$damage_binary,\n                     positive \u003d \"1\"\n                     )\n\n# accuracy\naccuracy  \u003c- conf_matrix$overall[\u0027Accuracy\u0027]\n\n# Positive class \u003d 1, precision, recall, and F1\n# Extract precision, recall, and F1 score\nprecision \u003c- conf_matrix$byClass[\u0027Precision\u0027]\nrecall \u003c- conf_matrix$byClass[\u0027Recall\u0027]\nf1_score \u003c- conf_matrix$byClass[\u0027F1\u0027]\n\n\n# Log parameters and metrics\n# mlflow_log_param(\"model_type\", \"scm-xgboost-classification\")\nmlflow_log_metric(\"accuracy\", accuracy)\nmlflow_log_metric(\"F1\", f1_score)\nmlflow_log_metric(\"Precision\", precision)\nmlflow_log_metric(\"Recall\", recall)\n\n\n# Save model\n#saveRDS(model, file \u003d file.path(path_2_folder, \"spam_clas_model.rds\"))\n\n# End MLflow run\nmlflow_end_run()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Recipe outputs\nmanaged_folder_path \u003c- dkuManagedFolderPath(\"xcPrnvPS\")\n\nsaveRDS(damage_fit_class_full, file \u003d paste0(managed_folder_path, \"/base_clas_full_model.rds\"))\n\nsaveRDS(base_wind_model, file \u003d paste0(managed_folder_path, \"/base_wind_model.rds\"))\n\nsaveRDS(base_rain_model, file \u003d paste0(managed_folder_path, \"/base_rain_model.rds\"))\nsaveRDS(base_track_model, file \u003d paste0(managed_folder_path, \"/base_track_model.rds\"))"
      ],
      "outputs": []
    }
  ]
}