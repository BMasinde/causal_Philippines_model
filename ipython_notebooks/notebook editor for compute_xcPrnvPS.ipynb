{
  "metadata": {
    "kernelspec": {
      "name": "r-dku-venv-r_env",
      "display_name": "R (env R_env)",
      "language": "R"
    },
    "hide_input": false,
    "language_info": {
      "name": "R",
      "codemirror_mode": "r",
      "pygments_lexer": "r",
      "mimetype": "text/x-r-source",
      "file_extension": ".r",
      "version": "4.4.1"
    },
    "createdOn": 1740376599429,
    "associatedRecipe": "compute_xcPrnvPS",
    "creationTag": {
      "versionNumber": 0,
      "lastModifiedBy": {
        "login": "admin"
      },
      "lastModifiedOn": 1740376599429
    },
    "customFields": {},
    "creator": "admin",
    "tags": [
      "recipe-editor"
    ],
    "dkuGit": {
      "lastInteraction": 0
    },
    "modifiedBy": "admin"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Turning warning messages off\noptions(warn \u003d 0) # i don\u0027t care for the messages\n\n# Libraries\nlibrary(dataiku)\nlibrary(rpart)\nlibrary(dplyr)\nlibrary(caret)\nlibrary(pROC) # For AUC calculation\nlibrary(data.table)\nlibrary(mlflow)\nlibrary(reticulate)\nlibrary(Matrix)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Recipe inputs\ndf_base_train \u003c- dkuReadDataset(\"base_train\", samplingMethod\u003d\"head\", nbRows\u003d100000)\ndf_base_validation \u003c- dkuReadDataset(\"base_validation\", samplingMethod\u003d\"head\", nbRows\u003d100000)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Training track_min_dist ~ island_groups\n# we will need to also include island_groups \n# in the final outcome prediction model to adjust for the confounding\n\nbase_track_model  \u003c- rpart(track_min_dist  ~ island_groups,\n                          data \u003d df_base_train,\n                          method \u003d \"anova\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Training structural equation for wind speed\n# wind_speed \u003d f(track_min_dist, eps)\n\n\nbase_wind_model \u003c- rpart(wind_max ~ track_min_dist,\n                       data \u003d df_base_train,\n                       method \u003d \"anova\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Training structural equation for rain speed\n# rain_total \u003d f(track_min_dist, eps)\n\nbase_rain_model \u003c- rpart(rain_total ~ track_min_dist,\n                       data \u003d df_base_train,\n                       method \u003d \"anova\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Adding the predicted parents\u0027 to the training dataset\n\ndf_base_train \u003c- df_base_train %\u003e%\n  mutate(track_min_dist_pred \u003d predict(base_track_model, newdata \u003d df_base_train), # predicted min_dist\n         wind_max_pred \u003d predict(base_wind_model, newdata \u003d df_base_train),\n         rain_total_pred \u003d predict(base_rain_model, newdata \u003d df_base_train), # Updating interaction terms\n         wind_blue_ss \u003d wind_max_pred * blue_ss_frac,\n         wind_yellow_ss \u003d wind_max_pred * yellow_ss_frac,\n         wind_orange_ss \u003d wind_max_pred * orange_ss_frac,\n         wind_red_ss \u003d wind_max_pred * red_ss_frac,\n         rain_blue_ss \u003d rain_total_pred * blue_ls_frac,\n         rain_yellow_ss \u003d rain_total_pred * yellow_ls_frac,\n         rain_orange_ss \u003d rain_total_pred * orange_ls_frac,\n         rain_red_ss \u003d rain_total_pred * red_ls_frac,\n         )"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Adding the predicted parents\u0027 to the validation dataset\n# predicting for wind and rainfall for the validation dataset\ndf_base_validation \u003c- df_base_validation %\u003e%\n  mutate(track_min_dist_pred \u003d predict(base_track_model, newdata \u003d df_base_validation),  # First predict for track_min_dist from regions\n    wind_max_pred \u003d predict(base_wind_model, newdata \u003d df_base_validation),\n    rain_total_pred \u003d predict(base_rain_model, newdata \u003d df_base_validation),\n    wind_blue_ss \u003d wind_max_pred * blue_ss_frac,\n    wind_yellow_ss \u003d wind_max_pred * yellow_ss_frac,\n    wind_orange_ss \u003d wind_max_pred * orange_ss_frac,\n    wind_red_ss \u003d wind_max_pred * red_ss_frac,\n    rain_blue_ss \u003d rain_total_pred * blue_ls_frac,\n    rain_yellow_ss \u003d rain_total_pred * yellow_ls_frac,\n    rain_orange_ss \u003d rain_total_pred * orange_ls_frac,\n    rain_red_ss \u003d rain_total_pred * red_ls_frac,\n  )\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Parameter tuning\n\n# Define tuning grid\ntune_grid \u003c- expand.grid(\n  nrounds \u003d c(50, 100, 150),       # Number of boosting rounds\n  max_depth \u003d c(3, 6, 9),          # Maximum tree depth\n  eta \u003d c(0.01, 0.1, 0.3),         # Learning rate\n  gamma \u003d 0,                       # Minimum loss reduction\n  colsample_bytree \u003d 0.8,          # Feature selection rate\n  min_child_weight \u003d 1,            # Minimum instance weight\n  subsample \u003d 0.8                  # Sample ratio per boosting round\n)\n\n\n# Create an empty list to store results\nresults_list \u003c- list()\n\n# Extra data prep\n# Ensure target variable is a factor for classification\ndf_base_train$damage_binary \u003c- as.factor(df_base_train$damage_binary)\ndf_base_validation$damage_binary \u003c- as.factor(df_base_validation$damage_binary)\n\n# Train the model using manual grid search\ngrid_id \u003c- 1  # Index for list storage\n\n# Iterate over all combinations of hyperparameters\nfor (i in 1:nrow(tune_grid)) {\n  params \u003c- tune_grid[i, ]\n       \n        # setting seed for reproducibility\n        set.seed(1234)\n        # Train the model with specific hyperparameters\n        xgb_model \u003c- train(\n          as.factor(damage_binary) ~ wind_max_pred +\n            rain_total_pred +\n            roof_strong_wall_strong +\n            roof_strong_wall_light +\n            roof_strong_wall_salv +\n            roof_light_wall_strong +\n            roof_light_wall_light +\n            roof_light_wall_salv +\n            roof_salv_wall_strong +\n            roof_salv_wall_light +\n            roof_salv_wall_salv +\n            ls_risk_pct +\n            ss_risk_pct +\n            wind_blue_ss +\n            wind_yellow_ss +\n            wind_orange_ss +\n            wind_red_ss +\n            rain_blue_ss +\n            rain_yellow_ss +\n            rain_orange_ss +\n            rain_red_ss +\n            island_groups, # CONFOUNDER ADJUSTED\n          data \u003d df_base_train,\n          method \u003d \"xgbTree\", # XGBoost method\n          trControl \u003d trainControl(method \u003d \"none\"),  # No automatic validation\n          tuneGrid \u003d params # Hyperparameter grid\n        )\n\n        # Make probability predictions for classification\n        val_predictions \u003c- predict(xgb_model, newdata \u003d df_base_validation, type \u003d \"prob\")[,2]  # Probability of class 1\n\n        # Compute AUC (better for classification)\n        auc_value \u003c- auc(df_base_validation$damage_binary, val_predictions)\n\n        # Store results efficiently in a list\n        results_list[[i]] \u003c- data.frame(params, AUC \u003d auc_value)\n}\n\n# Convert list to data frame\nresults \u003c- rbindlist(results_list)\n\n# Print the best hyperparameter combination (highest AUC)\nbest_params \u003c- results[which.max(results$AUC), ]\nprint(best_params)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "best_params"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Training based on tuned parameters\n\n# Combine Training and Validation datasets for final training\n\nfinal_training_df  \u003c- rbind(df_base_train,\n                           df_base_validation)\n\n\n# Extract the best parameters (remove AUC column)\nbest_params_model \u003c- best_params %\u003e% # Remove AUC column if present\n    select(-AUC)\n\ndamage_fit_class_full \u003c- xgb_model \u003c- train(\n          as.factor(damage_binary) ~ wind_max_pred +\n            rain_total_pred +\n            roof_strong_wall_strong +\n            roof_strong_wall_light +\n            roof_strong_wall_salv +\n            roof_light_wall_strong +\n            roof_light_wall_light +\n            roof_light_wall_salv +\n            roof_salv_wall_strong +\n            roof_salv_wall_light +\n            roof_salv_wall_salv +\n            ls_risk_pct +\n            ss_risk_pct +\n            wind_blue_ss +\n            wind_yellow_ss +\n            wind_orange_ss +\n            wind_red_ss +\n            rain_blue_ss +\n            rain_yellow_ss +\n            rain_orange_ss +\n            rain_red_ss +\n            island_groups,\n          data \u003d final_training_df, # USE TRAINING AND VALIDATION SETS COMBINED\n          method \u003d \"xgbTree\", # XGBoost method\n          trControl \u003d trainControl(method \u003d \"none\"),  # No automatic validation\n          tuneGrid \u003d best_params_model # USE BEST PARAMETER\n        )"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sanity Check\n# testing on the training datasets (training + validation)\n\n## Outcome prediction on the final_training_df dataset\n## default function predict returns class probabilities (has two columns)\ny_pred \u003c- predict(damage_fit_class_full,\n                  newdata \u003d final_training_df)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# using table function\nconf_matrix \u003c- confusionMatrix(y_pred,\n                     final_training_df$damage_binary,  \n                     positive \u003d \"1\"\n                     )\nconf_matrix"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracy \u003c- conf_matrix$overall[\u0027Accuracy\u0027]\n\ncat(\"test-set accuracy of minimal SCM model:\", accuracy, sep \u003d \" \")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Logging the model and parameter using MLflow\n\n# set tracking URI\nmlflow_set_tracking_uri(\"http://127.0.0.1:5000\")\n\n# Ensure any active run is ended\nsuppressWarnings(try(mlflow_end_run(), silent \u003d TRUE))\n\n# set experiment\n# Logging metrics for model training and the parameters used\nmlflow_set_experiment(experiment_name \u003d \"SCM - XGBOOST classification (Training metircs)\")\n\n# Start MLflow run\nmlflow_start_run(nested \u003d FALSE)\n\n# Ensure the run ends even if an error occurs\non.exit(mlflow_end_run(), add \u003d TRUE)\n\n# Extract the best parameters (remove AUC column)\nbest_params_model \u003c- best_params %\u003e% # Remove AUC column if present\n    select(-AUC)\n\ndamage_fit_class_full \u003c- xgb_model \u003c- train(\n          as.factor(damage_binary) ~ wind_max_pred +\n            rain_total_pred +\n            roof_strong_wall_strong +\n            roof_strong_wall_light +\n            roof_strong_wall_salv +\n            roof_light_wall_strong +\n            roof_light_wall_light +\n            roof_light_wall_salv +\n            roof_salv_wall_strong +\n            roof_salv_wall_light +\n            roof_salv_wall_salv +\n            ls_risk_pct +\n            ss_risk_pct +\n            wind_blue_ss +\n            wind_yellow_ss +\n            wind_orange_ss +\n            wind_red_ss +\n            rain_blue_ss +\n            rain_yellow_ss +\n            rain_orange_ss +\n            rain_red_ss +\n            island_groups,\n          data \u003d final_training_df, # USE TRAINING AND VALIDATION SETS COMBINED\n          method \u003d \"xgbTree\", # XGBoost method\n          trControl \u003d trainControl(method \u003d \"none\"),  # No automatic validation\n          tuneGrid \u003d best_params_model # USE BEST PARAMETER\n        )\n\n\n# summarize results\nconf_matrix \u003c- confusionMatrix(y_pred,\n                     final_training_df$damage_binary,  \n                     positive \u003d \"1\"\n                     )\n\n# accuracy\naccuracy  \u003c- conf_matrix$overall[\u0027Accuracy\u0027]\n\n# Positive class \u003d 1, precision, recall, and F1\n# Extract precision, recall, and F1 score\nprecision \u003c- conf_matrix$byClass[\u0027Precision\u0027]\nrecall \u003c- conf_matrix$byClass[\u0027Recall\u0027]\nf1_score \u003c- conf_matrix$byClass[\u0027F1\u0027]\n\n\n# Log parameters and metrics\nmlflow_log_param(\"model_type\", \"scm-xgboost-classification\")\nmlflow_log_metric(\"accuracy\", accuracy)\nmlflow_log_metric(\"F1\", F1)\nmlflow_log_metric(\"Precision\", precision)\nmlflow_log_metric(\"Recall\", recall)\n\n\n# Save model\n#saveRDS(model, file \u003d file.path(path_2_folder, \"spam_clas_model.rds\"))\n\n# End MLflow run\nmlflow_end_run()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Recipe outputs\nmanaged_folder_path \u003c- dkuManagedFolderPath(\"xcPrnvPS\")\n\nsaveRDS(damage_fit_class_full, file \u003d paste0(managed_folder_path, \"/base_clas_full_model.rds\"))\n\nsaveRDS(base_wind_model, file \u003d paste0(managed_folder_path, \"/base_wind_model.rds\"))\n\nsaveRDS(base_rain_model, file \u003d paste0(managed_folder_path, \"/base_rain_model.rds\"))\nsaveRDS(base_track_model, file \u003d paste0(managed_folder_path, \"/base_track_model.rds\"))"
      ],
      "outputs": []
    }
  ]
}