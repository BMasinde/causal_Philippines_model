{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "R (env R_env)",
      "language": "R",
      "name": "r-dku-venv-r_env"
    },
    "associatedRecipe": "compute_xcPrnvPS",
    "dkuGit": {
      "lastInteraction": 0
    },
    "creationTag": {
      "versionNumber": 0,
      "lastModifiedBy": {
        "login": "admin"
      },
      "lastModifiedOn": 1740376599429
    },
    "creator": "admin",
    "createdOn": 1740376599429,
    "tags": [
      "recipe-editor"
    ],
    "customFields": {}
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Turning warning messages off\n",
        "options(warn \u003d 0) # i don\u0027t care for the messages\n",
        "\n",
        "# Libraries\n",
        "library(dataiku)\n",
        "library(rpart)\n",
        "library(dplyr)\n",
        "library(caret)\n",
        "library(pROC) # For AUC calculation\n",
        "library(data.table)\n",
        "library(mlflow)\n",
        "library(reticulate)\n",
        "library(Matrix)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Recipe inputs\n",
        "df_base_train \u003c- dkuReadDataset(\"base_train\", samplingMethod\u003d\"head\", nbRows\u003d100000)\n",
        "df_base_validation \u003c- dkuReadDataset(\"base_validation\", samplingMethod\u003d\"head\", nbRows\u003d100000)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Training track_min_dist ~ island_groups\n",
        "# we will need to also include island_groups\n",
        "# in the final outcome prediction model to adjust for the confounding\n",
        "\n",
        "base_track_model  \u003c- rpart(track_min_dist  ~ island_groups,\n",
        "                          data \u003d df_base_train,\n",
        "                          method \u003d \"anova\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Training structural equation for wind speed\n",
        "# wind_speed \u003d f(track_min_dist, eps)\n",
        "\n",
        "\n",
        "base_wind_model \u003c- rpart(wind_max ~ track_min_dist,\n",
        "                       data \u003d df_base_train,\n",
        "                       method \u003d \"anova\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Training structural equation for rain speed\n",
        "# rain_total \u003d f(track_min_dist, eps)\n",
        "\n",
        "base_rain_model \u003c- rpart(rain_total ~ track_min_dist,\n",
        "                       data \u003d df_base_train,\n",
        "                       method \u003d \"anova\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Adding the predicted parents\u0027 to the training dataset\n",
        "\n",
        "df_base_train \u003c- df_base_train %\u003e%\n",
        "  mutate(track_min_dist_pred \u003d predict(base_track_model, newdata \u003d df_base_train), # predicted min_dist\n",
        "         wind_max_pred \u003d predict(base_wind_model, newdata \u003d df_base_train),\n",
        "         rain_total_pred \u003d predict(base_rain_model, newdata \u003d df_base_train), # Updating interaction terms\n",
        "         wind_blue_ss \u003d wind_max_pred * blue_ss_frac,\n",
        "         wind_yellow_ss \u003d wind_max_pred * yellow_ss_frac,\n",
        "         wind_orange_ss \u003d wind_max_pred * orange_ss_frac,\n",
        "         wind_red_ss \u003d wind_max_pred * red_ss_frac,\n",
        "         rain_blue_ss \u003d rain_total_pred * blue_ls_frac,\n",
        "         rain_yellow_ss \u003d rain_total_pred * yellow_ls_frac,\n",
        "         rain_orange_ss \u003d rain_total_pred * orange_ls_frac,\n",
        "         rain_red_ss \u003d rain_total_pred * red_ls_frac,\n",
        "         )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Adding the predicted parents\u0027 to the validation dataset\n",
        "# predicting for wind and rainfall for the validation dataset\n",
        "df_base_validation \u003c- df_base_validation %\u003e%\n",
        "  mutate(track_min_dist_pred \u003d predict(base_track_model, newdata \u003d df_base_validation),  # First predict for track_min_dist from regions\n",
        "    wind_max_pred \u003d predict(base_wind_model, newdata \u003d df_base_validation),\n",
        "    rain_total_pred \u003d predict(base_rain_model, newdata \u003d df_base_validation),\n",
        "    wind_blue_ss \u003d wind_max_pred * blue_ss_frac,\n",
        "    wind_yellow_ss \u003d wind_max_pred * yellow_ss_frac,\n",
        "    wind_orange_ss \u003d wind_max_pred * orange_ss_frac,\n",
        "    wind_red_ss \u003d wind_max_pred * red_ss_frac,\n",
        "    rain_blue_ss \u003d rain_total_pred * blue_ls_frac,\n",
        "    rain_yellow_ss \u003d rain_total_pred * yellow_ls_frac,\n",
        "    rain_orange_ss \u003d rain_total_pred * orange_ls_frac,\n",
        "    rain_red_ss \u003d rain_total_pred * red_ls_frac,\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Parameter tuning\n",
        "\n",
        "# Define tuning grid\n",
        "tune_grid \u003c- expand.grid(\n",
        "  nrounds \u003d c(50, 100, 150),       # Number of boosting rounds\n",
        "  max_depth \u003d c(3, 6, 9),          # Maximum tree depth\n",
        "  eta \u003d c(0.01, 0.1, 0.3),         # Learning rate\n",
        "  gamma \u003d 0,                       # Minimum loss reduction\n",
        "  colsample_bytree \u003d 0.8,          # Feature selection rate\n",
        "  min_child_weight \u003d 1,            # Minimum instance weight\n",
        "  subsample \u003d 0.8                  # Sample ratio per boosting round\n",
        ")\n",
        "\n",
        "\n",
        "# Create an empty list to store results\n",
        "results_list \u003c- list()\n",
        "\n",
        "# Extra data prep\n",
        "# Ensure target variable is a factor for classification\n",
        "df_base_train$damage_binary \u003c- as.factor(df_base_train$damage_binary)\n",
        "df_base_validation$damage_binary \u003c- as.factor(df_base_validation$damage_binary)\n",
        "\n",
        "# Train the model using manual grid search\n",
        "grid_id \u003c- 1  # Index for list storage\n",
        "\n",
        "# Iterate over all combinations of hyperparameters\n",
        "for (i in 1:nrow(tune_grid)) {\n",
        "  params \u003c- tune_grid[i, ]\n",
        "\n",
        "        # setting seed for reproducibility\n",
        "        set.seed(1234)\n",
        "        # Train the model with specific hyperparameters\n",
        "        xgb_model \u003c- train(\n",
        "          as.factor(damage_binary) ~ wind_max_pred +\n",
        "            rain_total_pred +\n",
        "            roof_strong_wall_strong +\n",
        "            roof_strong_wall_light +\n",
        "            roof_strong_wall_salv +\n",
        "            roof_light_wall_strong +\n",
        "            roof_light_wall_light +\n",
        "            roof_light_wall_salv +\n",
        "            roof_salv_wall_strong +\n",
        "            roof_salv_wall_light +\n",
        "            roof_salv_wall_salv +\n",
        "            ls_risk_pct +\n",
        "            ss_risk_pct +\n",
        "            wind_blue_ss +\n",
        "            wind_yellow_ss +\n",
        "            wind_orange_ss +\n",
        "            wind_red_ss +\n",
        "            rain_blue_ss +\n",
        "            rain_yellow_ss +\n",
        "            rain_orange_ss +\n",
        "            rain_red_ss +\n",
        "            island_groups, # CONFOUNDER ADJUSTED\n",
        "          data \u003d df_base_train,\n",
        "          method \u003d \"xgbTree\", # XGBoost method\n",
        "          trControl \u003d trainControl(method \u003d \"none\"),  # No automatic validation\n",
        "          tuneGrid \u003d params # Hyperparameter grid\n",
        "        )\n",
        "\n",
        "        # Make probability predictions for classification\n",
        "        val_predictions \u003c- predict(xgb_model, newdata \u003d df_base_validation, type \u003d \"prob\")[,2]  # Probability of class 1\n",
        "\n",
        "        # Compute AUC (better for classification)\n",
        "        auc_value \u003c- auc(df_base_validation$damage_binary, val_predictions)\n",
        "\n",
        "        # Store results efficiently in a list\n",
        "        results_list[[i]] \u003c- data.frame(params, AUC \u003d auc_value)\n",
        "}\n",
        "\n",
        "# Convert list to data frame\n",
        "results \u003c- rbindlist(results_list)\n",
        "\n",
        "# Print the best hyperparameter combination (highest AUC)\n",
        "best_params \u003c- results[which.max(results$AUC), ]\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "best_params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Training based on tuned parameters\n",
        "\n",
        "# Combine Training and Validation datasets for final training\n",
        "\n",
        "final_training_df  \u003c- rbind(df_base_train,\n",
        "                           df_base_validation)\n",
        "\n",
        "\n",
        "# Extract the best parameters (remove AUC column)\n",
        "best_params_model \u003c- best_params %\u003e% # Remove AUC column if present\n",
        "    select(-AUC)\n",
        "\n",
        "damage_fit_class_full \u003c- xgb_model \u003c- train(\n",
        "          as.factor(damage_binary) ~ wind_max_pred +\n",
        "            rain_total_pred +\n",
        "            roof_strong_wall_strong +\n",
        "            roof_strong_wall_light +\n",
        "            roof_strong_wall_salv +\n",
        "            roof_light_wall_strong +\n",
        "            roof_light_wall_light +\n",
        "            roof_light_wall_salv +\n",
        "            roof_salv_wall_strong +\n",
        "            roof_salv_wall_light +\n",
        "            roof_salv_wall_salv +\n",
        "            ls_risk_pct +\n",
        "            ss_risk_pct +\n",
        "            wind_blue_ss +\n",
        "            wind_yellow_ss +\n",
        "            wind_orange_ss +\n",
        "            wind_red_ss +\n",
        "            rain_blue_ss +\n",
        "            rain_yellow_ss +\n",
        "            rain_orange_ss +\n",
        "            rain_red_ss +\n",
        "            island_groups,\n",
        "          data \u003d final_training_df, # USE TRAINING AND VALIDATION SETS COMBINED\n",
        "          method \u003d \"xgbTree\", # XGBoost method\n",
        "          trControl \u003d trainControl(method \u003d \"none\"),  # No automatic validation\n",
        "          tuneGrid \u003d best_params_model # USE BEST PARAMETER\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Sanity Check\n",
        "# testing on the training datasets (training + validation)\n",
        "\n",
        "## Outcome prediction on the final_training_df dataset\n",
        "## default function predict returns class probabilities (has two columns)\n",
        "y_pred \u003c- predict(damage_fit_class_full,\n",
        "                  newdata \u003d final_training_df)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# using table function\n",
        "conf_matrix \u003c- confusionMatrix(y_pred,\n",
        "                     final_training_df$damage_binary,\n",
        "                     positive \u003d \"1\"\n",
        "                     )\n",
        "conf_matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "accuracy \u003c- conf_matrix$overall[\u0027Accuracy\u0027]\n",
        "\n",
        "cat(\"test-set accuracy of minimal SCM model:\", accuracy, sep \u003d \" \")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Logging the model and parameter using MLflow\n",
        "\n",
        "# set tracking URI\n",
        "mlflow_set_tracking_uri(\"http://127.0.0.1:5000\")\n",
        "\n",
        "# Ensure any active run is ended\n",
        "suppressWarnings(try(mlflow_end_run(), silent \u003d TRUE))\n",
        "\n",
        "# set experiment\n",
        "# Logging metrics for model training and the parameters used\n",
        "mlflow_set_experiment(experiment_name \u003d \"SCM - XGBOOST classification (Training metircs)\")\n",
        "\n",
        "# Ensure that MLflow has only one run. Start MLflow run once.\n",
        "run_name \u003c- paste(\"XGBoost Run\", Sys.time())  # Unique name using current time\n",
        "\n",
        "\n",
        "# Start MLflow run\n",
        "mlflow_start_run(nested \u003d FALSE)\n",
        "\n",
        "# Ensure the run ends even if an error occurs\n",
        "#on.exit(mlflow_end_run(), add \u003d TRUE)\n",
        "\n",
        "# Extract the best parameters (remove AUC column)\n",
        "best_params_model \u003c- best_params %\u003e% # Remove AUC column if present\n",
        "    select(-AUC)\n",
        "\n",
        "# Log each of the best parameters in MLflow\n",
        "for (param in names(best_params_model)) {\n",
        "  mlflow_log_param(param, best_params_model[[param]])\n",
        "}\n",
        "\n",
        "# Log the model type as a parameter\n",
        "mlflow_log_param(\"model_type\", \"scm-xgboost-classification\")\n",
        "\n",
        "damage_fit_class_full \u003c- xgb_model \u003c- train(\n",
        "          as.factor(damage_binary) ~ wind_max_pred +\n",
        "            rain_total_pred +\n",
        "            roof_strong_wall_strong +\n",
        "            roof_strong_wall_light +\n",
        "            roof_strong_wall_salv +\n",
        "            roof_light_wall_strong +\n",
        "            roof_light_wall_light +\n",
        "            roof_light_wall_salv +\n",
        "            roof_salv_wall_strong +\n",
        "            roof_salv_wall_light +\n",
        "            roof_salv_wall_salv +\n",
        "            ls_risk_pct +\n",
        "            ss_risk_pct +\n",
        "            wind_blue_ss +\n",
        "            wind_yellow_ss +\n",
        "            wind_orange_ss +\n",
        "            wind_red_ss +\n",
        "            rain_blue_ss +\n",
        "            rain_yellow_ss +\n",
        "            rain_orange_ss +\n",
        "            rain_red_ss +\n",
        "            island_groups,\n",
        "          data \u003d final_training_df, # USE TRAINING AND VALIDATION SETS COMBINED\n",
        "          method \u003d \"xgbTree\", # XGBoost method\n",
        "          trControl \u003d trainControl(method \u003d \"none\"),  # No automatic validation\n",
        "          tuneGrid \u003d best_params_model # USE BEST PARAMETER\n",
        "        )\n",
        "\n",
        "\n",
        "# summarize results\n",
        "conf_matrix \u003c- confusionMatrix(y_pred,\n",
        "                     final_training_df$damage_binary,\n",
        "                     positive \u003d \"1\"\n",
        "                     )\n",
        "\n",
        "# accuracy\n",
        "accuracy  \u003c- conf_matrix$overall[\u0027Accuracy\u0027]\n",
        "\n",
        "# Positive class \u003d 1, precision, recall, and F1\n",
        "# Extract precision, recall, and F1 score\n",
        "precision \u003c- conf_matrix$byClass[\u0027Precision\u0027]\n",
        "recall \u003c- conf_matrix$byClass[\u0027Recall\u0027]\n",
        "f1_score \u003c- conf_matrix$byClass[\u0027F1\u0027]\n",
        "\n",
        "\n",
        "# Log parameters and metrics\n",
        "# mlflow_log_param(\"model_type\", \"scm-xgboost-classification\")\n",
        "mlflow_log_metric(\"accuracy\", accuracy)\n",
        "mlflow_log_metric(\"F1\", f1_score)\n",
        "mlflow_log_metric(\"Precision\", precision)\n",
        "mlflow_log_metric(\"Recall\", recall)\n",
        "\n",
        "\n",
        "# Save model\n",
        "#saveRDS(model, file \u003d file.path(path_2_folder, \"spam_clas_model.rds\"))\n",
        "\n",
        "# End MLflow run\n",
        "mlflow_end_run()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Recipe outputs\n",
        "managed_folder_path \u003c- dkuManagedFolderPath(\"xcPrnvPS\")\n",
        "\n",
        "saveRDS(damage_fit_class_full, file \u003d paste0(managed_folder_path, \"/base_clas_full_model.rds\"))\n",
        "\n",
        "saveRDS(base_wind_model, file \u003d paste0(managed_folder_path, \"/base_wind_model.rds\"))\n",
        "\n",
        "saveRDS(base_rain_model, file \u003d paste0(managed_folder_path, \"/base_rain_model.rds\"))\n",
        "saveRDS(base_track_model, file \u003d paste0(managed_folder_path, \"/base_track_model.rds\"))"
      ]
    }
  ]
}