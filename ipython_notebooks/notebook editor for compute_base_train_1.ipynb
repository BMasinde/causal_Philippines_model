{
  "metadata": {
    "kernelspec": {
      "name": "r-dku-venv-r_4_4_1",
      "display_name": "R (env R_4_4_1)",
      "language": "R"
    },
    "hide_input": false,
    "language_info": {
      "name": "R",
      "codemirror_mode": "r",
      "pygments_lexer": "r",
      "mimetype": "text/x-r-source",
      "file_extension": ".r",
      "version": "4.4.1"
    },
    "tags": [
      "recipe-editor"
    ],
    "associatedRecipe": "compute_base_train",
    "customFields": {},
    "creator": "admin",
    "createdOn": 1740066610508,
    "modifiedBy": "admin"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train, Test, and Validation Dataset Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We do a 60/20/20 split to create the datasets. Because we are going to use the hurdle method for predictions we split the modeling data into two categories: base and truncated. No additional processing (filtering) is required to create \"base_\" datasets. To create \"truncated_\" datasets we filter the modeling data by outcome variable (damage_perc \u003e\u003d 10). "
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Libraries\nlibrary(dataiku)\nlibrary(dplyr)\n\n# Recipe inputs\nmodeling_data \u003c- dkuReadDataset(\"modeling_data\", samplingMethod\u003d\"head\", nbRows\u003d100000)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# splits for base_ datasets\n\n# number of rows in modeling_data\nn \u003c- nrow(modeling_data)\n\n# Seeding for reproducibility\nset.seed(12345)\n\n# Generate random indices for 60% training set\nbase_train_id \u003c- sample(1:n, floor(n * 0.6))\n\n# Remaining indices after training selection\nbase_remaining_id \u003c- setdiff(1:n, base_train_id)\n\n# Split remaining 40% into 20% validation and 20% test\nbase_val_id \u003c- sample(base_remaining_id, floor(n * 0.2))\n\nbase_test_id \u003c- setdiff(base_remaining_id, base_val_id)  # The rest goes to test\n\n\n# Compute recipe outputs for base_ datasets\nbase_train \u003c- modeling_data[base_train_id, ] # Compute a data frame for the output to write into base_train\n\nbase_test \u003c- modeling_data[base_test_id, ] # Compute a data frame for the output to write into base_test\n\nbase_validation \u003c- modeling_data[base_val_id, ] # Compute a data frame for the output to write into base_validation"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# splits for truncated_ datasets\n\n# Filtering modeling data by column damage_perc \u003e\u003d 10\ntruncated_data \u003c- modeling_data %\u003e%\n  filter(damage_perc \u003e\u003d 10)\n\n# Reset row ID\u0027s\nrownames(truncated_data) \u003c- 1:nrow(truncated_data)\n\n# number of observations with damage \u003e 10\nn_trunc \u003c- nrow(truncated_data)\n\n# Use 60/20/20 split\ntrunc_train_id \u003c- sample(1:n_trunc, \n                  floor(n_trunc*0.6)\n                  )\n\n# Get the remaining 40% indices\ntrunc_remaining_id \u003c- setdiff(1:n_trunc, trunc_train_id)\n\n# Randomly select 50% of the remaining (which is 20% of the total) for validation\ntrunc_val_id \u003c- sample(trunc_remaining_id, floor(n_trunc * 0.2 / 0.4 * length(trunc_remaining_id)))\n\n## The rest (remaining 20%) goes to test\ntrunc_test_id \u003c- setdiff(trunc_remaining_id, trunc_val_id)\n\n\n# Compute recipe outputs for truncated_ datasets\ntruncated_train \u003c- truncated_data[trunc_train_id, ] # Compute a data frame for the output to write into truncated_train\ntruncated_validation \u003c- truncated_data[trunc_val_id, ] # Compute a data frame for the output to write into truncated_validation\ntruncated_test \u003c- truncated_data[trunc_test_id, ] # Compute a data frame for the output to write into truncated_test"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "nrow(truncated_data)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "nrow(modeling_data)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Recipe outputs\ndkuWriteDataset(base_train,\"base_train\")\ndkuWriteDataset(base_test,\"base_test\")\ndkuWriteDataset(base_validation,\"base_validation\")\ndkuWriteDataset(truncated_train,\"truncated_train\")\ndkuWriteDataset(truncated_validation,\"truncated_validation\")\ndkuWriteDataset(truncated_test,\"truncated_test\")"
      ],
      "outputs": []
    }
  ]
}